{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jzmrRj8lzJIv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import argparse\n",
        "import datetime\n",
        "import time\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HMyxN-ZLzQW_"
      },
      "outputs": [],
      "source": [
        "def get_loader(args):\n",
        "    if args.dataset == 'mnist':\n",
        "        # Transforms for train\n",
        "        train_transform = transforms.Compose([transforms.Resize([args.image_size, args.image_size]),\n",
        "                                            transforms.RandomCrop(args.image_size, padding=2),\n",
        "                                            transforms.ToTensor(),\n",
        "                                            transforms.Normalize([0.5], [0.5])])\n",
        "        train = datasets.MNIST(os.path.join(args.data_path, args.dataset), train=True, download=True, transform=train_transform)\n",
        "\n",
        "        # Transforms for test\n",
        "        test_transform = transforms.Compose([transforms.Resize([args.image_size, args.image_size]), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])])\n",
        "        test = datasets.MNIST(os.path.join(args.data_path, args.dataset), train=False, download=True, transform=test_transform)\n",
        "\n",
        "\n",
        "    elif args.dataset == 'fmnist':\n",
        "        train_transform = transforms.Compose([transforms.Resize([args.image_size, args.image_size]),\n",
        "                                            transforms.RandomCrop(args.image_size, padding=2),\n",
        "                                            transforms.RandomHorizontalFlip(),\n",
        "                                            transforms.ToTensor(),\n",
        "                                            transforms.Normalize([0.5], [0.5])])\n",
        "        train = datasets.FashionMNIST(os.path.join(args.data_path, args.dataset), train=True, download=True, transform=train_transform)\n",
        "\n",
        "        test_transform = transforms.Compose([transforms.Resize([args.image_size, args.image_size]), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])])\n",
        "        test = datasets.FashionMNIST(os.path.join(args.data_path, args.dataset), train=False, download=True, transform=test_transform)\n",
        "\n",
        "\n",
        "    elif args.dataset == 'svhn':\n",
        "        train_transform = transforms.Compose([transforms.Resize([args.image_size, args.image_size]),\n",
        "                                            transforms.RandomCrop(args.image_size, padding=2),\n",
        "                                            transforms.ToTensor(),\n",
        "                                            transforms.Normalize([0.4376821, 0.4437697, 0.47280442], [0.19803012, 0.20101562, 0.19703614])])\n",
        "        train = datasets.SVHN(os.path.join(args.data_path, args.dataset), split='train', download=True, transform=train_transform)\n",
        "\n",
        "        test_transform = transforms.Compose([transforms.Resize([args.image_size, args.image_size]), transforms.ToTensor(), transforms.Normalize([0.4376821, 0.4437697, 0.47280442], [0.19803012, 0.20101562, 0.19703614])])\n",
        "        test = datasets.SVHN(os.path.join(args.data_path, args.dataset), split='test', download=True, transform=test_transform)\n",
        "\n",
        "    elif args.dataset == 'cifar10':\n",
        "        train_transform = transforms.Compose([transforms.Resize([args.image_size, args.image_size]),\n",
        "                                            transforms.RandomCrop(args.image_size, padding=4),\n",
        "                                            transforms.RandomHorizontalFlip(),\n",
        "                                            transforms.RandAugment(),  # RandAugment augmentation for strong regularization\n",
        "                                            transforms.ToTensor(),\n",
        "                                            transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2470, 0.2435, 0.2616])])\n",
        "        train = datasets.CIFAR10(os.path.join(args.data_path, args.dataset), train=True, download=True, transform=train_transform)\n",
        "\n",
        "        test_transform = transforms.Compose([transforms.Resize([args.image_size, args.image_size]), transforms.ToTensor(), transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2470, 0.2435, 0.2616])])\n",
        "        test = datasets.CIFAR10(os.path.join(args.data_path, args.dataset), train=False, download=True, transform=test_transform)\n",
        "\n",
        "    else:\n",
        "        print(\"Unknown dataset\")\n",
        "        exit(0)\n",
        "\n",
        "    # Define dataloaders\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train,\n",
        "                                                 batch_size=args.batch_size,\n",
        "                                                 shuffle=True,\n",
        "                                                 num_workers=args.n_workers,\n",
        "                                                 drop_last=True)\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(dataset=test,\n",
        "                                                batch_size=args.batch_size,\n",
        "                                                shuffle=False,\n",
        "                                                num_workers=args.n_workers,\n",
        "                                                drop_last=False)\n",
        "\n",
        "    return train_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YLGZ3fsKzagU"
      },
      "outputs": [],
      "source": [
        "# B -> Batch Size\n",
        "# C -> Number of Input Channels\n",
        "# IH -> Image Height\n",
        "# IW -> Image Width\n",
        "# P -> Patch Size\n",
        "# E -> Embedding Dimension\n",
        "# S -> Sequence Length = IH/P * IW/P\n",
        "# Q -> Query Sequence length (same as S for self-attention)\n",
        "# K -> Key Sequence length (same as S for self-attention)\n",
        "# V -> Value Sequence length (same as S for self-attention)\n",
        "# H -> Number of heads\n",
        "# HE -> Head Embedding Dimension = E/H\n",
        "\n",
        "\n",
        "class EmbedLayer(nn.Module):\n",
        "    def __init__(self, n_channels, embed_dim, image_size, patch_size, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(n_channels, embed_dim, kernel_size=patch_size, stride=patch_size)  # Pixel Encoding\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim), requires_grad=True)  # Cls Token\n",
        "        self.pos_embedding = nn.Parameter(torch.zeros(1, (image_size // patch_size) ** 2 + 1, embed_dim), requires_grad=True)  # Positional Embedding\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)  # B C IH IW -> B E IH/P IW/P (Embedding the patches)\n",
        "        x = x.reshape([x.shape[0], x.shape[1], -1])  # B E IH/P IW/P -> B E S (Flattening the patches)\n",
        "        x = x.transpose(1, 2)  # B E S -> B S E\n",
        "        x = torch.cat((torch.repeat_interleave(self.cls_token, x.shape[0], 0), x), dim=1)  # Adding classification token at the start of every sequence\n",
        "        x = x + self.pos_embedding  # Adding positional embedding\n",
        "\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, n_attention_heads):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.n_attention_heads = n_attention_heads\n",
        "        self.head_embed_dim = embed_dim // n_attention_heads\n",
        "\n",
        "        self.queries = nn.Linear(self.embed_dim, self.head_embed_dim * self.n_attention_heads) # Queries projection\n",
        "        self.keys = nn.Linear(self.embed_dim, self.head_embed_dim * self.n_attention_heads)    # Keys projection\n",
        "        self.values = nn.Linear(self.embed_dim, self.head_embed_dim * self.n_attention_heads)  # Values projection\n",
        "\n",
        "        self.out_projection = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, s, e = x.shape\n",
        "\n",
        "        xq = self.queries(x).reshape(b, s, self.n_attention_heads, self.head_embed_dim)  # B, Q, E -> B, Q, H, HE\n",
        "        xq = xq.transpose(1, 2)  # B, Q, H, HE -> B, H, Q, HE\n",
        "        xk = self.keys(x).reshape(b, s, self.n_attention_heads, self.head_embed_dim)  # B, K, E -> B, K, H, HE\n",
        "        xk = xk.transpose(1, 2)  # B, K, H, HE -> B, H, K, HE\n",
        "        xv = self.values(x).reshape(b, s, self.n_attention_heads, self.head_embed_dim)  # B, V, E -> B, V, H, HE\n",
        "        xv = xv.transpose(1, 2)  # B, V, H, HE -> B, H, V, HE\n",
        "\n",
        "        # Compute Attention presoftmax values\n",
        "        xk = xk.transpose(-1, -2)  # B, H, K, HE -> B, H, HE, K\n",
        "        x_attention = torch.matmul(xq, xk)  # B, H, Q, HE  *  B, H, HE, K -> B, H, Q, K\n",
        "\n",
        "        # Scale presoftmax values for stability\n",
        "        x_attention /= float(self.head_embed_dim) ** 0.5\n",
        "\n",
        "        # Compute Attention Matrix\n",
        "        x_attention = torch.softmax(x_attention, dim=-1)\n",
        "\n",
        "        # Compute Attention Values\n",
        "        x = torch.matmul(x_attention, xv)  # B, H, Q, K * B, H, V, HE -> B, H, Q, HE\n",
        "\n",
        "        # Format the output\n",
        "        x = x.transpose(1, 2)  # B, H, Q, HE -> B, Q, H, HE\n",
        "        x = x.reshape(b, s, e)  # B, Q, H, HE -> B, Q, E\n",
        "\n",
        "        x = self.out_projection(x)  # B, Q, E -> B, Q, E\n",
        "        return x\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, embed_dim, n_attention_heads, forward_mul, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.attention = SelfAttention(embed_dim, n_attention_heads)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.fc1 = nn.Linear(embed_dim, embed_dim * forward_mul)\n",
        "        self.activation = nn.GELU()\n",
        "        self.fc2 = nn.Linear(embed_dim * forward_mul, embed_dim)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.dropout1(self.attention(self.norm1(x))) # Skip connections\n",
        "        x = x + self.dropout2(self.fc2(self.activation(self.fc1(self.norm2(x)))))  # Skip connections\n",
        "        return x\n",
        "\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, embed_dim, n_classes):\n",
        "        super().__init__()\n",
        "        # New architectures skip fc1 and activations and directly apply fc2.\n",
        "        self.fc1 = nn.Linear(embed_dim, embed_dim)\n",
        "        self.activation = nn.Tanh()\n",
        "        self.fc2 = nn.Linear(embed_dim, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x[:, 0, :]  # Get CLS token\n",
        "        x = self.fc1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, n_channels, embed_dim, n_layers, n_attention_heads, forward_mul, image_size, patch_size, n_classes, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embedding = EmbedLayer(n_channels, embed_dim, image_size, patch_size, dropout=dropout)\n",
        "        self.encoder = nn.ModuleList([Encoder(embed_dim, n_attention_heads, forward_mul, dropout=dropout) for _ in range(n_layers)])\n",
        "        self.norm = nn.LayerNorm(embed_dim) # Final normalization layer after the last block\n",
        "        self.classifier = Classifier(embed_dim, n_classes)\n",
        "\n",
        "\n",
        "        self.apply(vit_init_weights) # Weight initalization\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        for block in self.encoder:\n",
        "            x = block(x)\n",
        "        x = self.norm(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def vit_init_weights(m):\n",
        "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "        nn.init.trunc_normal_(m.weight, mean=0.0, std=0.02)\n",
        "        if m.bias is not None:\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    elif isinstance(m, nn.LayerNorm):\n",
        "        nn.init.constant_(m.weight, 1)\n",
        "        nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    elif isinstance(m, EmbedLayer):\n",
        "        nn.init.trunc_normal_(m.cls_token, mean=0.0, std=0.02)\n",
        "        nn.init.trunc_normal_(m.pos_embedding, mean=0.0, std=0.02)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "x3cv6xGvoXof"
      },
      "outputs": [],
      "source": [
        "class Solver(object):\n",
        "\tdef __init__(self, args):\n",
        "\t\tself.args = args\n",
        "\n",
        "\t\t# Get data loaders\n",
        "\t\tself.train_loader, self.test_loader = get_loader(args)\n",
        "\n",
        "\t\t# Create object of the Vision Transformer\n",
        "\t\tself.model = VisionTransformer(n_channels=self.args.n_channels, embed_dim=self.args.embed_dim,\n",
        "\t\t\t\t\t\t\t\t\t\tn_layers=self.args.n_layers, n_attention_heads=self.args.n_attention_heads,\n",
        "\t\t\t\t\t\t\t\t\t\tforward_mul=self.args.forward_mul, image_size=self.args.image_size,\n",
        "\t\t\t\t\t\t\t\t\t\tpatch_size=self.args.patch_size, n_classes=self.args.n_classes, dropout=self.args.dropout)\n",
        "\n",
        "\t\t# Push to GPU\n",
        "\t\tif self.args.is_cuda:\n",
        "\t\t\tprint(\"Using GPU\")\n",
        "\t\t\tself.model = self.model.cuda()\n",
        "\t\telse:\n",
        "\t\t\tprint(\"Cuda not available.\")\n",
        "\n",
        "\t\t# Display Vision Transformer\n",
        "\t\tprint('--------Network--------')\n",
        "\t\tprint(self.model)\n",
        "\n",
        "\t\t# Option to load pretrained model\n",
        "\t\tif args.load_model:\n",
        "\t\t\tprint(\"Using pretrained model\")\n",
        "\t\t\tself.model.load_state_dict(torch.load(os.path.join(self.args.model_path, 'ViT_model.pt')))\n",
        "\n",
        "\t\t# Training loss function\n",
        "\t\tself.loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "\t\t# Arrays to record training progression\n",
        "\t\tself.train_losses = []\n",
        "\t\tself.test_losses = []\n",
        "\t\tself.train_accuracies = []\n",
        "\t\tself.test_accuracies = []\n",
        "\n",
        "\tdef test_dataset(self, loader):\n",
        "\t\t# Set Vision Transformer to evaluation mode\n",
        "\t\tself.model.eval()\n",
        "\n",
        "\t\t# Arrays to record all labels and logits\n",
        "\t\tall_labels = []\n",
        "\t\tall_logits = []\n",
        "\n",
        "\t\t# Testing loop\n",
        "\t\tfor (x, y) in loader:\n",
        "\t\t\tif self.args.is_cuda:\n",
        "\t\t\t\tx = x.cuda()\n",
        "\n",
        "\t\t\t# Avoid capturing gradients in evaluation time for faster speed\n",
        "\t\t\twith torch.no_grad():\n",
        "\t\t\t\tlogits = self.model(x)\n",
        "\n",
        "\t\t\tall_labels.append(y)\n",
        "\t\t\tall_logits.append(logits.cpu())\n",
        "\n",
        "\t\t# Convert all captured variables to torch\n",
        "\t\tall_labels = torch.cat(all_labels)\n",
        "\t\tall_logits = torch.cat(all_logits)\n",
        "\t\tall_pred = all_logits.max(1)[1]\n",
        "\n",
        "\t\t# Compute loss, accuracy and confusion matrix\n",
        "\t\tloss = self.loss_fn(all_logits, all_labels).item()\n",
        "\t\tacc = accuracy_score(y_true=all_labels, y_pred=all_pred)\n",
        "\t\tcm = confusion_matrix(y_true=all_labels, y_pred=all_pred, labels=range(self.args.n_classes))\n",
        "\n",
        "\t\treturn acc, cm, loss\n",
        "\n",
        "\tdef test(self, train=True):\n",
        "\t\tif train:\n",
        "\t\t\t# Test using train loader\n",
        "\t\t\tacc, cm, loss = self.test_dataset(self.train_loader)\n",
        "\t\t\tprint(f\"Train acc: {acc:.2%}\\tTrain loss: {loss:.4f}\\nTrain Confusion Matrix:\")\n",
        "\t\t\tprint(cm)\n",
        "\n",
        "\t\t# Test using test loader\n",
        "\t\tacc, cm, loss = self.test_dataset(self.test_loader)\n",
        "\t\tprint(f\"Test acc: {acc:.2%}\\tTest loss: {loss:.4f}\\nTrain Confusion Matrix:\")\n",
        "\t\tprint(cm)\n",
        "\n",
        "\t\treturn acc, loss\n",
        "\n",
        "\tdef train(self):\n",
        "\t\titers_per_epoch = len(self.train_loader)\n",
        "\n",
        "\t\t# Define optimizer for training the model\n",
        "\t\toptimizer = optim.AdamW(self.model.parameters(), lr=self.args.lr, weight_decay=1e-3)\n",
        "\n",
        "\t\t# scheduler for linear warmup of lr and then cosine decay\n",
        "\t\tlinear_warmup = optim.lr_scheduler.LinearLR(optimizer, start_factor=1/self.args.warmup_epochs, end_factor=1.0, total_iters=self.args.warmup_epochs-1, last_epoch=-1, verbose=True)\n",
        "\t\tcos_decay = optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=self.args.epochs-self.args.warmup_epochs, eta_min=1e-5, verbose=True)\n",
        "\n",
        "\t\t# Variable to capture best test accuracy\n",
        "\t\tbest_acc = 0\n",
        "\n",
        "\t\t# Training loop\n",
        "\t\tfor epoch in range(self.args.epochs):\n",
        "\n",
        "\t\t\t# Set model to training mode\n",
        "\t\t\tself.model.train()\n",
        "\n",
        "\t\t\t# Arrays to record epoch loss and accuracy\n",
        "\t\t\ttrain_epoch_loss = []\n",
        "\t\t\ttrain_epoch_accuracy = []\n",
        "\n",
        "\t\t\t# Loop on loader\n",
        "\t\t\tfor i, (x, y) in enumerate(self.train_loader):\n",
        "\n",
        "\t\t\t\t# Push to GPU\n",
        "\t\t\t\tif self.args.is_cuda:\n",
        "\t\t\t\t\tx, y = x.cuda(), y.cuda()\n",
        "\n",
        "\t\t\t\t# Get output logits from the model\n",
        "\t\t\t\tlogits = self.model(x)\n",
        "\n",
        "\t\t\t\t# Compute training loss\n",
        "\t\t\t\tloss = self.loss_fn(logits, y)\n",
        "\n",
        "\t\t\t\t# Updating the model\n",
        "\t\t\t\toptimizer.zero_grad()\n",
        "\t\t\t\tloss.backward()\n",
        "\t\t\t\toptimizer.step()\n",
        "\n",
        "\t\t\t\t# Batch metrics\n",
        "\t\t\t\tbatch_pred = logits.max(1)[1]\n",
        "\t\t\t\tbatch_accuracy = (y==batch_pred).float().mean()\n",
        "\t\t\t\ttrain_epoch_loss += [loss.item()]\n",
        "\t\t\t\ttrain_epoch_accuracy += [batch_accuracy.item()]\n",
        "\n",
        "\t\t\t\t# Log training progress\n",
        "\t\t\t\tif i % 50 == 0 or i == (iters_per_epoch - 1):\n",
        "\t\t\t\t\tprint(f'Ep: {epoch+1}/{self.args.epochs}\\tIt: {i+1}/{iters_per_epoch}\\tbatch_loss: {loss:.4f}\\tbatch_accuracy: {batch_accuracy:.2%}')\n",
        "\n",
        "\t\t\t# Test the test set after every epoch\n",
        "\t\t\ttest_acc, test_loss = self.test(train=((epoch+1)%25==0)) # Test training set every 25 epochs\n",
        "\n",
        "\t\t\t# Capture best test accuracy\n",
        "\t\t\tbest_acc = max(test_acc, best_acc)\n",
        "\t\t\tprint(f\"Best test acc: {best_acc:.2%}\\n\")\n",
        "\n",
        "\t\t\t# Save model\n",
        "\t\t\ttorch.save(self.model.state_dict(), os.path.join(self.args.model_path, \"ViT_model.pt\"))\n",
        "\n",
        "\t\t\t# Update learning rate using schedulers\n",
        "\t\t\tif epoch < self.args.warmup_epochs:\n",
        "\t\t\t\tlinear_warmup.step()\n",
        "\t\t\telse:\n",
        "\t\t\t\tcos_decay.step()\n",
        "\n",
        "\t\t\t# Update training progression metric arrays\n",
        "\t\t\tself.train_losses += [sum(train_epoch_loss)/iters_per_epoch]\n",
        "\t\t\tself.test_losses += [test_loss]\n",
        "\t\t\tself.train_accuracies += [sum(train_epoch_accuracy)/iters_per_epoch]\n",
        "\t\t\tself.test_accuracies += [test_acc]\n",
        "\n",
        "\tdef plot_mnist_predictions(self):\n",
        "\t\t# Download MNIST test set\n",
        "\t\t# mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
        "\t\t# mnist_loader = torch.utils.data.DataLoader(mnist_test, batch_size=10, shuffle=True)\n",
        "\n",
        "\t\t# Get 10 random samples from MNIST test set\n",
        "\t\ttest_ld = iter(self.test_loader)\n",
        "\t\timages, labels = next(test_ld)\n",
        "\n",
        "\t\t# Move images to the device\n",
        "\t\tif self.args.is_cuda:\n",
        "\t\t\timages = images.cuda()\n",
        "\n",
        "\t\t# Generate predictions\n",
        "\t\t# with torch.no_grad():\n",
        "\t\tself.model.eval()\n",
        "\t\tpredictions = self.model(images).argmax(dim=1).cpu().numpy()\n",
        "\n",
        "\t\t# Plot images with predictions\n",
        "\t\tfig, axes = plt.subplots(2, 5, figsize=(12, 6))\n",
        "\t\tfig.suptitle('MNIST Samples with Predictions', fontsize=16)\n",
        "\n",
        "\t\tfor i, (image, label, prediction) in enumerate(zip(images, labels, predictions)):\n",
        "\t\t\tax = axes[i // 5, i % 5]\n",
        "\t\t\tax.imshow(image.squeeze().cpu().numpy(), cmap='gray')\n",
        "\t\t\tax.set_title(f\"Label: {label}, Prediction: {prediction}\")\n",
        "\t\t\tax.axis('off')\n",
        "\n",
        "\t\tplt.tight_layout()\n",
        "\t\tplt.show()\n",
        "\n",
        "\tdef plot_graphs(self):\n",
        "\t\t# Plot graph of loss values\n",
        "\t\tplt.plot(self.train_losses, color='b', label='Train')\n",
        "\t\tplt.plot(self.test_losses, color='r', label='Test')\n",
        "\n",
        "\t\tplt.ylabel('Loss', fontsize = 18)\n",
        "\t\tplt.yticks(fontsize=16)\n",
        "\t\tplt.xlabel('Epoch', fontsize = 18)\n",
        "\t\tplt.xticks(fontsize=16)\n",
        "\t\tplt.legend(fontsize=15, frameon=False)\n",
        "\n",
        "\t\t# plt.show()  # Option to view graph while training\n",
        "\t\tplt.savefig(os.path.join(self.args.output_path, 'graph_loss.png'), bbox_inches='tight')\n",
        "\t\tplt.close('all')\n",
        "\n",
        "\n",
        "\t\t# Plot graph of accuracies\n",
        "\t\tplt.plot(self.train_accuracies, color='b', label='Train')\n",
        "\t\tplt.plot(self.test_accuracies, color='r', label='Test')\n",
        "\n",
        "\t\tplt.ylabel('Accuracy', fontsize = 18)\n",
        "\t\tplt.yticks(fontsize=16)\n",
        "\t\tplt.xlabel('Epoch', fontsize = 18)\n",
        "\t\tplt.xticks(fontsize=16)\n",
        "\t\tplt.legend(fontsize=15, frameon=False)\n",
        "\n",
        "\t\t# plt.show()  # Option to view graph while training\n",
        "\t\tplt.savefig(os.path.join(self.args.output_path, 'graph_accuracy.png'), bbox_inches='tight')\n",
        "\t\tplt.close('all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "d0Brxtgk1vfa"
      },
      "outputs": [],
      "source": [
        "def main(args):\n",
        "    # Create required directories if they don't exist\n",
        "    os.makedirs(args.model_path, exist_ok=True)\n",
        "    os.makedirs(args.output_path, exist_ok=True)\n",
        "\n",
        "    solver = Solver(args)\n",
        "    solver.train()                  # Training function\n",
        "    solver.plot_graphs()            # Training plots\n",
        "    solver.test(train=True)         # Testing function\n",
        "\n",
        "\n",
        "# Print arguments\n",
        "def print_args(args):\n",
        "    for k in dict(sorted(vars(args).items())).items():\n",
        "        print(k)\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhwvZgGLPwS4",
        "outputId": "751727e2-580c-4d6e-fa51-2af7cef8e2fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Started at 2024-05-11 06:23:29\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 37000127.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/mnist/MNIST/raw/train-images-idx3-ubyte.gz to ./data/mnist/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 3926602.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/mnist/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 10345867.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/mnist/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 10636811.15it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/mnist/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using GPU\n",
            "--------Network--------\n",
            "VisionTransformer(\n",
            "  (embedding): EmbedLayer(\n",
            "    (conv1): Conv2d(1, 64, kernel_size=(4, 4), stride=(4, 4))\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (encoder): ModuleList(\n",
            "    (0-5): 6 x Encoder(\n",
            "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      (attention): SelfAttention(\n",
            "        (queries): Linear(in_features=64, out_features=64, bias=True)\n",
            "        (keys): Linear(in_features=64, out_features=64, bias=True)\n",
            "        (values): Linear(in_features=64, out_features=64, bias=True)\n",
            "        (out_projection): Linear(in_features=64, out_features=64, bias=True)\n",
            "      )\n",
            "      (dropout1): Dropout(p=0.1, inplace=False)\n",
            "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      (fc1): Linear(in_features=64, out_features=128, bias=True)\n",
            "      (activation): GELU(approximate='none')\n",
            "      (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
            "      (dropout2): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "  (classifier): Classifier(\n",
            "    (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
            "    (activation): Tanh()\n",
            "    (fc2): Linear(in_features=64, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ep: 1/20\tIt: 1/468\tbatch_loss: 2.3036\tbatch_accuracy: 5.47%\n",
            "Ep: 1/20\tIt: 51/468\tbatch_loss: 2.3016\tbatch_accuracy: 11.72%\n",
            "Ep: 1/20\tIt: 101/468\tbatch_loss: 2.2687\tbatch_accuracy: 17.97%\n",
            "Ep: 1/20\tIt: 151/468\tbatch_loss: 2.2569\tbatch_accuracy: 16.41%\n",
            "Ep: 1/20\tIt: 201/468\tbatch_loss: 2.1952\tbatch_accuracy: 26.56%\n",
            "Ep: 1/20\tIt: 251/468\tbatch_loss: 2.1826\tbatch_accuracy: 19.53%\n",
            "Ep: 1/20\tIt: 301/468\tbatch_loss: 2.1701\tbatch_accuracy: 14.84%\n",
            "Ep: 1/20\tIt: 351/468\tbatch_loss: 2.0561\tbatch_accuracy: 18.75%\n",
            "Ep: 1/20\tIt: 401/468\tbatch_loss: 1.9895\tbatch_accuracy: 29.69%\n",
            "Ep: 1/20\tIt: 451/468\tbatch_loss: 2.0084\tbatch_accuracy: 23.44%\n",
            "Ep: 1/20\tIt: 468/468\tbatch_loss: 2.0435\tbatch_accuracy: 20.31%\n",
            "Test acc: 27.52%\tTest loss: 1.9427\n",
            "Train Confusion Matrix:\n",
            "[[ 662    4  163   78    0    0   21   30    0   22]\n",
            " [   2 1085    4    1    0    0    1   40    0    2]\n",
            " [ 606    3  235  121    0    0   18   25    0   24]\n",
            " [ 543    3  268  119    0    0   17   28    0   32]\n",
            " [  29  223   59  111    0    0   64  383    0  113]\n",
            " [ 414    1  269  130    0    0   20   32    0   26]\n",
            " [ 327   16  229  188    0    0   54   75    0   69]\n",
            " [  33  204   65  122    0    0   42  427    0  135]\n",
            " [ 331    8  287  207    0    0   28   66    0   47]\n",
            " [  50   70  111  206    0    0   69  333    0  170]]\n",
            "Best test acc: 27.52%\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ep: 2/20\tIt: 1/468\tbatch_loss: 1.9906\tbatch_accuracy: 25.78%\n",
            "Ep: 2/20\tIt: 51/468\tbatch_loss: 1.9158\tbatch_accuracy: 32.81%\n",
            "Ep: 2/20\tIt: 101/468\tbatch_loss: 1.7795\tbatch_accuracy: 32.03%\n",
            "Ep: 2/20\tIt: 151/468\tbatch_loss: 1.7217\tbatch_accuracy: 32.81%\n",
            "Ep: 2/20\tIt: 201/468\tbatch_loss: 1.7103\tbatch_accuracy: 37.50%\n",
            "Ep: 2/20\tIt: 251/468\tbatch_loss: 1.7397\tbatch_accuracy: 37.50%\n",
            "Ep: 2/20\tIt: 301/468\tbatch_loss: 1.7123\tbatch_accuracy: 39.84%\n",
            "Ep: 2/20\tIt: 351/468\tbatch_loss: 1.5565\tbatch_accuracy: 42.97%\n",
            "Ep: 2/20\tIt: 401/468\tbatch_loss: 1.5098\tbatch_accuracy: 38.28%\n",
            "Ep: 2/20\tIt: 451/468\tbatch_loss: 1.4252\tbatch_accuracy: 49.22%\n",
            "Ep: 2/20\tIt: 468/468\tbatch_loss: 1.4273\tbatch_accuracy: 42.97%\n",
            "Test acc: 48.40%\tTest loss: 1.3775\n",
            "Train Confusion Matrix:\n",
            "[[ 878    1   31   29    4   19    5    1   11    1]\n",
            " [   2 1113    0    2    0    5    1    8    2    2]\n",
            " [ 214    0  116  407    1  202   26    7   56    3]\n",
            " [  39    0   34  360    0  540    7    5   14   11]\n",
            " [   9   88    0    0  394    2   42  327    3  117]\n",
            " [  11    3   11  222    0  618    7    6    8    6]\n",
            " [ 284   15   10   49   31   53  249   22  160   85]\n",
            " [   3  129    2    8   83   50   26  596    9  122]\n",
            " [ 292    5   51  248    5   87   86    6  185    9]\n",
            " [  15   32    2    7   28   20   69  498    7  331]]\n",
            "Best test acc: 48.40%\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ep: 3/20\tIt: 1/468\tbatch_loss: 1.4736\tbatch_accuracy: 50.00%\n",
            "Ep: 3/20\tIt: 51/468\tbatch_loss: 1.3689\tbatch_accuracy: 48.44%\n",
            "Ep: 3/20\tIt: 101/468\tbatch_loss: 1.4047\tbatch_accuracy: 53.12%\n",
            "Ep: 3/20\tIt: 151/468\tbatch_loss: 1.2990\tbatch_accuracy: 49.22%\n",
            "Ep: 3/20\tIt: 201/468\tbatch_loss: 1.2953\tbatch_accuracy: 49.22%\n",
            "Ep: 3/20\tIt: 251/468\tbatch_loss: 1.3120\tbatch_accuracy: 51.56%\n",
            "Ep: 3/20\tIt: 301/468\tbatch_loss: 1.1123\tbatch_accuracy: 54.69%\n",
            "Ep: 3/20\tIt: 351/468\tbatch_loss: 1.0712\tbatch_accuracy: 57.81%\n",
            "Ep: 3/20\tIt: 401/468\tbatch_loss: 1.0623\tbatch_accuracy: 60.16%\n",
            "Ep: 3/20\tIt: 451/468\tbatch_loss: 1.0982\tbatch_accuracy: 63.28%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ep: 3/20\tIt: 468/468\tbatch_loss: 0.9857\tbatch_accuracy: 62.50%\n",
            "Test acc: 63.95%\tTest loss: 0.9951\n",
            "Train Confusion Matrix:\n",
            "[[ 758    0    9   16   85    2   71    4   19   16]\n",
            " [   0 1110    0    4    4    3    0   12    1    1]\n",
            " [   4    0  231  484    3   69    8   72  142   19]\n",
            " [   0    3   21  725    0  185    1   65    8    2]\n",
            " [   0    6    0    0  866    0    6   20    0   84]\n",
            " [   1    2    8  554    0  258    0   50    8   11]\n",
            " [   5    7   14   13  245    9  482    4  119   60]\n",
            " [   0    9    0    7   26   14    0  949    1   22]\n",
            " [  12    2  161  134   11   16   84   23  469   62]\n",
            " [   1    2    0    9   92    5    3  344    6  547]]\n",
            "Best test acc: 63.95%\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ep: 4/20\tIt: 1/468\tbatch_loss: 1.1063\tbatch_accuracy: 55.47%\n",
            "Ep: 4/20\tIt: 51/468\tbatch_loss: 1.1184\tbatch_accuracy: 56.25%\n",
            "Ep: 4/20\tIt: 101/468\tbatch_loss: 0.9359\tbatch_accuracy: 71.09%\n",
            "Ep: 4/20\tIt: 151/468\tbatch_loss: 0.9844\tbatch_accuracy: 57.03%\n",
            "Ep: 4/20\tIt: 201/468\tbatch_loss: 0.9949\tbatch_accuracy: 67.97%\n",
            "Ep: 4/20\tIt: 251/468\tbatch_loss: 1.0599\tbatch_accuracy: 67.97%\n",
            "Ep: 4/20\tIt: 301/468\tbatch_loss: 0.8386\tbatch_accuracy: 69.53%\n",
            "Ep: 4/20\tIt: 351/468\tbatch_loss: 0.7474\tbatch_accuracy: 74.22%\n",
            "Ep: 4/20\tIt: 401/468\tbatch_loss: 0.8406\tbatch_accuracy: 66.41%\n",
            "Ep: 4/20\tIt: 451/468\tbatch_loss: 0.8073\tbatch_accuracy: 75.78%\n",
            "Ep: 4/20\tIt: 468/468\tbatch_loss: 0.6827\tbatch_accuracy: 75.78%\n",
            "Test acc: 79.73%\tTest loss: 0.6073\n",
            "Train Confusion Matrix:\n",
            "[[ 944    0    0    1   10    0   10    5    4    6]\n",
            " [   0 1119    1    8    0    0    1    6    0    0]\n",
            " [  13    0  741   91    2   17   13   12  129   14]\n",
            " [   3    3   35  790    0  143    0   23    9    4]\n",
            " [   1    9    0    0  916    1   10    8    0   37]\n",
            " [  11    2   13  525    1  315    3    7   10    5]\n",
            " [  26    6   14    1   82    3  788    0   36    2]\n",
            " [   0   19    6   35    4    4    0  951    2    7]\n",
            " [  34    5   88   13    1    5   69    6  703   50]\n",
            " [   5    6    2    8   65   16    0  196    5  706]]\n",
            "Best test acc: 79.73%\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ep: 5/20\tIt: 1/468\tbatch_loss: 0.7072\tbatch_accuracy: 74.22%\n",
            "Ep: 5/20\tIt: 51/468\tbatch_loss: 0.8441\tbatch_accuracy: 73.44%\n",
            "Ep: 5/20\tIt: 101/468\tbatch_loss: 0.9795\tbatch_accuracy: 68.75%\n",
            "Ep: 5/20\tIt: 151/468\tbatch_loss: 0.7739\tbatch_accuracy: 75.00%\n",
            "Ep: 5/20\tIt: 201/468\tbatch_loss: 0.7917\tbatch_accuracy: 77.34%\n",
            "Ep: 5/20\tIt: 251/468\tbatch_loss: 0.5450\tbatch_accuracy: 82.81%\n",
            "Ep: 5/20\tIt: 301/468\tbatch_loss: 0.6127\tbatch_accuracy: 81.25%\n",
            "Ep: 5/20\tIt: 351/468\tbatch_loss: 0.5439\tbatch_accuracy: 84.38%\n",
            "Ep: 5/20\tIt: 401/468\tbatch_loss: 0.6863\tbatch_accuracy: 81.25%\n",
            "Ep: 5/20\tIt: 451/468\tbatch_loss: 0.6776\tbatch_accuracy: 78.91%\n",
            "Ep: 5/20\tIt: 468/468\tbatch_loss: 0.5610\tbatch_accuracy: 82.03%\n",
            "Test acc: 89.58%\tTest loss: 0.3801\n",
            "Train Confusion Matrix:\n",
            "[[ 939    1    2    0    8    8   11    2    3    6]\n",
            " [   0 1118    3    3    0    2    4    5    0    0]\n",
            " [   9    1  886   39    3    5   10   34   34   11]\n",
            " [   1    3   12  757    0  183    1   41    7    5]\n",
            " [   0    1    0    0  931    0    5    8    0   37]\n",
            " [   4    1    2   31    1  829    3    7    6    8]\n",
            " [   8    5    1    0   54   11  872    0    7    0]\n",
            " [   1    7    7    5    1    1    0 1001    0    5]\n",
            " [  26    3   11   12    5   20   26   10  771   90]\n",
            " [   6    1    0    2   40   16    0   84    6  854]]\n",
            "Best test acc: 89.58%\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ep: 6/20\tIt: 1/468\tbatch_loss: 0.7596\tbatch_accuracy: 75.78%\n",
            "Ep: 6/20\tIt: 51/468\tbatch_loss: 0.5136\tbatch_accuracy: 83.59%\n",
            "Ep: 6/20\tIt: 101/468\tbatch_loss: 0.4419\tbatch_accuracy: 85.16%\n",
            "Ep: 6/20\tIt: 151/468\tbatch_loss: 0.5424\tbatch_accuracy: 82.81%\n",
            "Ep: 6/20\tIt: 201/468\tbatch_loss: 0.4129\tbatch_accuracy: 86.72%\n",
            "Ep: 6/20\tIt: 251/468\tbatch_loss: 0.4351\tbatch_accuracy: 89.84%\n",
            "Ep: 6/20\tIt: 301/468\tbatch_loss: 0.3872\tbatch_accuracy: 85.16%\n",
            "Ep: 6/20\tIt: 351/468\tbatch_loss: 0.4539\tbatch_accuracy: 82.03%\n",
            "Ep: 6/20\tIt: 401/468\tbatch_loss: 0.4737\tbatch_accuracy: 85.16%\n",
            "Ep: 6/20\tIt: 451/468\tbatch_loss: 0.4080\tbatch_accuracy: 90.62%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ep: 6/20\tIt: 468/468\tbatch_loss: 0.3708\tbatch_accuracy: 87.50%\n",
            "Test acc: 93.41%\tTest loss: 0.2387\n",
            "Train Confusion Matrix:\n",
            "[[ 963    0    3    1    1    1    5    0    4    2]\n",
            " [   5 1106    4    4    1    1    5    4    3    2]\n",
            " [   5    1  977   22    1    0    0    8   13    5]\n",
            " [   1    2    7  970    0   11    0    8    9    2]\n",
            " [   1    4    0    0  906    0   11    4    1   55]\n",
            " [   3    1    2  128    0  721    6    2   26    3]\n",
            " [   9    5    5    1    3    6  921    0    8    0]\n",
            " [   3    9   20   19    0    0    0  959    2   16]\n",
            " [  21    0    9   19    1    2   16    1  898    7]\n",
            " [  11    4    0   11   13    6    1   13   30  920]]\n",
            "Best test acc: 93.41%\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ep: 7/20\tIt: 1/468\tbatch_loss: 0.2277\tbatch_accuracy: 92.97%\n",
            "Ep: 7/20\tIt: 51/468\tbatch_loss: 0.5633\tbatch_accuracy: 86.72%\n",
            "Ep: 7/20\tIt: 101/468\tbatch_loss: 0.3306\tbatch_accuracy: 89.06%\n",
            "Ep: 7/20\tIt: 151/468\tbatch_loss: 0.3294\tbatch_accuracy: 90.62%\n",
            "Ep: 7/20\tIt: 201/468\tbatch_loss: 0.4132\tbatch_accuracy: 88.28%\n",
            "Ep: 7/20\tIt: 251/468\tbatch_loss: 0.2769\tbatch_accuracy: 92.19%\n",
            "Ep: 7/20\tIt: 301/468\tbatch_loss: 0.2767\tbatch_accuracy: 90.62%\n",
            "Ep: 7/20\tIt: 351/468\tbatch_loss: 0.3348\tbatch_accuracy: 91.41%\n",
            "Ep: 7/20\tIt: 401/468\tbatch_loss: 0.2172\tbatch_accuracy: 95.31%\n",
            "Ep: 7/20\tIt: 451/468\tbatch_loss: 0.3599\tbatch_accuracy: 88.28%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ep: 7/20\tIt: 468/468\tbatch_loss: 0.2006\tbatch_accuracy: 94.53%\n",
            "Test acc: 95.16%\tTest loss: 0.1725\n",
            "Train Confusion Matrix:\n",
            "[[ 964    0    7    1    2    0    1    1    2    2]\n",
            " [   0 1120    6    2    0    0    4    3    0    0]\n",
            " [   2    0 1018    4    1    0    0    3    0    4]\n",
            " [   1    2   16  967    0   15    0    6    3    0]\n",
            " [   1    1    2    1  894    0    4    1    1   77]\n",
            " [   2    2    1   22    0  850    5    1    7    2]\n",
            " [   8    5    4    0   15    9  907    0   10    0]\n",
            " [   1    7   17   18    0    0    0  969    0   16]\n",
            " [  17    1   32   17    2    3    8    0  888    6]\n",
            " [   7    3    1   18    5   10    0    8   18  939]]\n",
            "Best test acc: 95.16%\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ep: 8/20\tIt: 1/468\tbatch_loss: 0.3510\tbatch_accuracy: 90.62%\n",
            "Ep: 8/20\tIt: 51/468\tbatch_loss: 0.3054\tbatch_accuracy: 92.97%\n",
            "Ep: 8/20\tIt: 101/468\tbatch_loss: 0.3980\tbatch_accuracy: 89.84%\n",
            "Ep: 8/20\tIt: 151/468\tbatch_loss: 0.1884\tbatch_accuracy: 94.53%\n",
            "Ep: 8/20\tIt: 201/468\tbatch_loss: 0.2832\tbatch_accuracy: 89.84%\n",
            "Ep: 8/20\tIt: 251/468\tbatch_loss: 0.2465\tbatch_accuracy: 92.19%\n",
            "Ep: 8/20\tIt: 301/468\tbatch_loss: 0.2498\tbatch_accuracy: 93.75%\n",
            "Ep: 8/20\tIt: 351/468\tbatch_loss: 0.2607\tbatch_accuracy: 90.62%\n",
            "Ep: 8/20\tIt: 401/468\tbatch_loss: 0.2411\tbatch_accuracy: 92.19%\n",
            "Ep: 8/20\tIt: 451/468\tbatch_loss: 0.2993\tbatch_accuracy: 93.75%\n",
            "Ep: 8/20\tIt: 468/468\tbatch_loss: 0.2444\tbatch_accuracy: 91.41%\n",
            "Test acc: 96.08%\tTest loss: 0.1316\n",
            "Train Confusion Matrix:\n",
            "[[ 968    0    2    0    1    0    6    1    2    0]\n",
            " [   1 1119    5    2    1    0    4    3    0    0]\n",
            " [   7    0 1013    2    2    0    2    3    2    1]\n",
            " [   2    1   18  962    0    7    0   10    9    1]\n",
            " [   0    0    0    0  950    0   11    0    2   19]\n",
            " [   5    1    2   16    0  849    9    2    5    3]\n",
            " [   4    4    1    0    4    4  941    0    0    0]\n",
            " [   0    7   14    3    0    0    0  990    4   10]\n",
            " [  14    0   12    1    5    6   19    2  911    4]\n",
            " [   4    2    1    6   52    6    0    9   24  905]]\n",
            "Best test acc: 96.08%\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ep: 9/20\tIt: 1/468\tbatch_loss: 0.3153\tbatch_accuracy: 90.62%\n",
            "Ep: 9/20\tIt: 51/468\tbatch_loss: 0.2710\tbatch_accuracy: 90.62%\n",
            "Ep: 9/20\tIt: 101/468\tbatch_loss: 0.2839\tbatch_accuracy: 90.62%\n",
            "Ep: 9/20\tIt: 151/468\tbatch_loss: 0.3176\tbatch_accuracy: 89.06%\n",
            "Ep: 9/20\tIt: 201/468\tbatch_loss: 0.1504\tbatch_accuracy: 96.88%\n",
            "Ep: 9/20\tIt: 251/468\tbatch_loss: 0.1756\tbatch_accuracy: 96.09%\n",
            "Ep: 9/20\tIt: 301/468\tbatch_loss: 0.2636\tbatch_accuracy: 91.41%\n",
            "Ep: 9/20\tIt: 351/468\tbatch_loss: 0.2077\tbatch_accuracy: 95.31%\n",
            "Ep: 9/20\tIt: 401/468\tbatch_loss: 0.2609\tbatch_accuracy: 92.97%\n",
            "Ep: 9/20\tIt: 451/468\tbatch_loss: 0.1792\tbatch_accuracy: 95.31%\n",
            "Ep: 9/20\tIt: 468/468\tbatch_loss: 0.2062\tbatch_accuracy: 92.97%\n",
            "Test acc: 96.46%\tTest loss: 0.1185\n",
            "Train Confusion Matrix:\n",
            "[[ 966    0    2    0    1    0    5    1    2    3]\n",
            " [   0 1111    3    2    1    1    4    9    0    4]\n",
            " [   2    0 1004    4    2    0    3    6    6    5]\n",
            " [   0    1   10  968    0   12    0   15    2    2]\n",
            " [   1    0    0    0  926    0    8    0    1   46]\n",
            " [   2    1    1   14    0  859    2    3    4    6]\n",
            " [   3    4    0    0    4   14  931    0    2    0]\n",
            " [   0    1    3    0    2    0    0 1009    1   12]\n",
            " [  16    0    4    8    3   10    2    5  889   37]\n",
            " [   1    0    0    6    7    3    0    9    0  983]]\n",
            "Best test acc: 96.46%\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ep: 10/20\tIt: 1/468\tbatch_loss: 0.2031\tbatch_accuracy: 95.31%\n",
            "Ep: 10/20\tIt: 51/468\tbatch_loss: 0.3122\tbatch_accuracy: 89.84%\n",
            "Ep: 10/20\tIt: 101/468\tbatch_loss: 0.1567\tbatch_accuracy: 95.31%\n",
            "Ep: 10/20\tIt: 151/468\tbatch_loss: 0.1419\tbatch_accuracy: 94.53%\n",
            "Ep: 10/20\tIt: 201/468\tbatch_loss: 0.1667\tbatch_accuracy: 92.97%\n",
            "Ep: 10/20\tIt: 251/468\tbatch_loss: 0.1151\tbatch_accuracy: 96.09%\n",
            "Ep: 10/20\tIt: 301/468\tbatch_loss: 0.2592\tbatch_accuracy: 89.84%\n",
            "Ep: 10/20\tIt: 351/468\tbatch_loss: 0.1729\tbatch_accuracy: 93.75%\n",
            "Ep: 10/20\tIt: 401/468\tbatch_loss: 0.1669\tbatch_accuracy: 96.88%\n",
            "Ep: 10/20\tIt: 451/468\tbatch_loss: 0.1778\tbatch_accuracy: 94.53%\n",
            "Ep: 10/20\tIt: 468/468\tbatch_loss: 0.1419\tbatch_accuracy: 96.88%\n",
            "Test acc: 97.10%\tTest loss: 0.0967\n",
            "Train Confusion Matrix:\n",
            "[[ 959    0    4    0    2    2   10    1    2    0]\n",
            " [   1 1113    4    0    1    0    6    2    8    0]\n",
            " [   1    0 1016    1    3    0    1    4    5    1]\n",
            " [   0    0    6  960    0   24    0   13    5    2]\n",
            " [   1    0    0    0  947    0    7    0    1   26]\n",
            " [   3    0    0    8    0  869    7    2    2    1]\n",
            " [   3    4    1    0    5    4  940    0    1    0]\n",
            " [   0    4    4    0    0    1    0 1008    1   10]\n",
            " [   9    0    8    1    2    5    6    3  930   10]\n",
            " [   3    0    0    4   12   12    0    7    3  968]]\n",
            "Best test acc: 97.10%\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ep: 11/20\tIt: 1/468\tbatch_loss: 0.2230\tbatch_accuracy: 95.31%\n",
            "Ep: 11/20\tIt: 51/468\tbatch_loss: 0.1196\tbatch_accuracy: 94.53%\n",
            "Ep: 11/20\tIt: 101/468\tbatch_loss: 0.2497\tbatch_accuracy: 93.75%\n",
            "Ep: 11/20\tIt: 151/468\tbatch_loss: 0.1826\tbatch_accuracy: 96.09%\n",
            "Ep: 11/20\tIt: 201/468\tbatch_loss: 0.1140\tbatch_accuracy: 96.09%\n",
            "Ep: 11/20\tIt: 251/468\tbatch_loss: 0.2828\tbatch_accuracy: 92.97%\n",
            "Ep: 11/20\tIt: 301/468\tbatch_loss: 0.1635\tbatch_accuracy: 94.53%\n",
            "Ep: 11/20\tIt: 351/468\tbatch_loss: 0.1363\tbatch_accuracy: 95.31%\n",
            "Ep: 11/20\tIt: 401/468\tbatch_loss: 0.2037\tbatch_accuracy: 92.19%\n",
            "Ep: 11/20\tIt: 451/468\tbatch_loss: 0.1199\tbatch_accuracy: 95.31%\n",
            "Ep: 11/20\tIt: 468/468\tbatch_loss: 0.1451\tbatch_accuracy: 95.31%\n",
            "Test acc: 96.79%\tTest loss: 0.0983\n",
            "Train Confusion Matrix:\n",
            "[[ 957    0    1    0    2    0   18    1    1    0]\n",
            " [   0 1121    4    1    1    0    7    1    0    0]\n",
            " [   3    1 1012    1    2    0    4    3    6    0]\n",
            " [   0    3   10  980    0    6    0    7    3    1]\n",
            " [   0    0    0    0  956    0    8    0    5   13]\n",
            " [   4    2    1    8    0  862    9    3    3    0]\n",
            " [   1    2    0    0    3    3  949    0    0    0]\n",
            " [   0   12   13    1    2    0    0  984    2   14]\n",
            " [   6    1   12    3    1    6   16    3  925    1]\n",
            " [   2    3    0    7   21    7    0    6   30  933]]\n",
            "Best test acc: 97.10%\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ep: 12/20\tIt: 1/468\tbatch_loss: 0.1747\tbatch_accuracy: 95.31%\n",
            "Ep: 12/20\tIt: 51/468\tbatch_loss: 0.1577\tbatch_accuracy: 94.53%\n",
            "Ep: 12/20\tIt: 101/468\tbatch_loss: 0.2124\tbatch_accuracy: 92.19%\n",
            "Ep: 12/20\tIt: 151/468\tbatch_loss: 0.2265\tbatch_accuracy: 92.19%\n",
            "Ep: 12/20\tIt: 201/468\tbatch_loss: 0.1570\tbatch_accuracy: 96.88%\n",
            "Ep: 12/20\tIt: 251/468\tbatch_loss: 0.2049\tbatch_accuracy: 92.97%\n",
            "Ep: 12/20\tIt: 301/468\tbatch_loss: 0.1723\tbatch_accuracy: 93.75%\n",
            "Ep: 12/20\tIt: 351/468\tbatch_loss: 0.1461\tbatch_accuracy: 96.88%\n",
            "Ep: 12/20\tIt: 401/468\tbatch_loss: 0.1226\tbatch_accuracy: 94.53%\n",
            "Ep: 12/20\tIt: 451/468\tbatch_loss: 0.0754\tbatch_accuracy: 98.44%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ep: 12/20\tIt: 468/468\tbatch_loss: 0.1749\tbatch_accuracy: 93.75%\n",
            "Test acc: 97.26%\tTest loss: 0.0905\n",
            "Train Confusion Matrix:\n",
            "[[ 974    0    1    0    0    0    2    1    2    0]\n",
            " [   1 1113    4    3    0    1    4    5    4    0]\n",
            " [   3    0 1011    5    0    0    0    8    5    0]\n",
            " [   0    0    4  978    0   14    0    8    4    2]\n",
            " [   4    0    0    0  914    0    7    0    2   55]\n",
            " [   3    0    1    6    0  868    1    3    9    1]\n",
            " [  11    2    0    0    0   11  932    0    2    0]\n",
            " [   1    1    2    1    1    0    0 1016    1    5]\n",
            " [   4    0    4    2    2    5    0    3  949    5]\n",
            " [   5    0    0    8    7    4    0    9    5  971]]\n",
            "Best test acc: 97.26%\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ep: 13/20\tIt: 1/468\tbatch_loss: 0.1312\tbatch_accuracy: 95.31%\n",
            "Ep: 13/20\tIt: 51/468\tbatch_loss: 0.0821\tbatch_accuracy: 97.66%\n",
            "Ep: 13/20\tIt: 101/468\tbatch_loss: 0.0874\tbatch_accuracy: 97.66%\n",
            "Ep: 13/20\tIt: 151/468\tbatch_loss: 0.1244\tbatch_accuracy: 96.09%\n",
            "Ep: 13/20\tIt: 201/468\tbatch_loss: 0.1486\tbatch_accuracy: 94.53%\n",
            "Ep: 13/20\tIt: 251/468\tbatch_loss: 0.1437\tbatch_accuracy: 95.31%\n",
            "Ep: 13/20\tIt: 301/468\tbatch_loss: 0.1928\tbatch_accuracy: 94.53%\n",
            "Ep: 13/20\tIt: 351/468\tbatch_loss: 0.1253\tbatch_accuracy: 96.09%\n",
            "Ep: 13/20\tIt: 401/468\tbatch_loss: 0.1348\tbatch_accuracy: 95.31%\n",
            "Ep: 13/20\tIt: 451/468\tbatch_loss: 0.2189\tbatch_accuracy: 92.97%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ep: 13/20\tIt: 468/468\tbatch_loss: 0.0866\tbatch_accuracy: 97.66%\n",
            "Test acc: 97.40%\tTest loss: 0.0775\n",
            "Train Confusion Matrix:\n",
            "[[ 967    1    2    0    1    1    7    1    0    0]\n",
            " [   0 1124    4    0    1    0    2    3    1    0]\n",
            " [   1    0 1024    1    1    0    1    1    3    0]\n",
            " [   0    1    5  997    0    2    0    3    2    0]\n",
            " [   1    1    0    0  961    0   12    0    0    7]\n",
            " [   0    4    1   16    0  857    5    2    6    1]\n",
            " [   4    4    0    0    1    3  945    0    1    0]\n",
            " [   1    5   35    5    2    0    0  970    3    7]\n",
            " [   2    0    9    2    3    1    5    0  951    1]\n",
            " [   3    3    0    5   36    3    1    6    8  944]]\n",
            "Best test acc: 97.40%\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ep: 14/20\tIt: 1/468\tbatch_loss: 0.1960\tbatch_accuracy: 92.97%\n",
            "Ep: 14/20\tIt: 51/468\tbatch_loss: 0.1339\tbatch_accuracy: 96.09%\n",
            "Ep: 14/20\tIt: 101/468\tbatch_loss: 0.1424\tbatch_accuracy: 96.09%\n",
            "Ep: 14/20\tIt: 151/468\tbatch_loss: 0.0433\tbatch_accuracy: 97.66%\n",
            "Ep: 14/20\tIt: 201/468\tbatch_loss: 0.1661\tbatch_accuracy: 95.31%\n",
            "Ep: 14/20\tIt: 251/468\tbatch_loss: 0.1142\tbatch_accuracy: 96.88%\n",
            "Ep: 14/20\tIt: 301/468\tbatch_loss: 0.0911\tbatch_accuracy: 98.44%\n",
            "Ep: 14/20\tIt: 351/468\tbatch_loss: 0.0828\tbatch_accuracy: 98.44%\n",
            "Ep: 14/20\tIt: 401/468\tbatch_loss: 0.1498\tbatch_accuracy: 95.31%\n",
            "Ep: 14/20\tIt: 451/468\tbatch_loss: 0.0811\tbatch_accuracy: 98.44%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ep: 14/20\tIt: 468/468\tbatch_loss: 0.1199\tbatch_accuracy: 95.31%\n",
            "Test acc: 97.74%\tTest loss: 0.0688\n",
            "Train Confusion Matrix:\n",
            "[[ 975    1    1    0    1    0    1    1    0    0]\n",
            " [   1 1126    2    1    1    0    2    2    0    0]\n",
            " [   8    0 1010    4    3    0    1    2    4    0]\n",
            " [   0    0    4 1003    0    1    0    1    1    0]\n",
            " [   1    1    0    0  944    0    2    1    2   31]\n",
            " [   2    1    1   22    0  860    1    2    2    1]\n",
            " [   6    4    0    0    7    7  933    0    1    0]\n",
            " [   0    6   13    8    0    0    0  994    2    5]\n",
            " [  11    0    3    2    0    3    1    1  948    5]\n",
            " [   2    1    0    8    7    3    0    7    0  981]]\n",
            "Best test acc: 97.74%\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ep: 15/20\tIt: 1/468\tbatch_loss: 0.1221\tbatch_accuracy: 96.88%\n",
            "Ep: 15/20\tIt: 51/468\tbatch_loss: 0.0623\tbatch_accuracy: 98.44%\n",
            "Ep: 15/20\tIt: 101/468\tbatch_loss: 0.1383\tbatch_accuracy: 95.31%\n",
            "Ep: 15/20\tIt: 151/468\tbatch_loss: 0.1188\tbatch_accuracy: 96.09%\n",
            "Ep: 15/20\tIt: 201/468\tbatch_loss: 0.0536\tbatch_accuracy: 98.44%\n",
            "Ep: 15/20\tIt: 251/468\tbatch_loss: 0.1631\tbatch_accuracy: 96.09%\n",
            "Ep: 15/20\tIt: 301/468\tbatch_loss: 0.0971\tbatch_accuracy: 96.88%\n",
            "Ep: 15/20\tIt: 351/468\tbatch_loss: 0.1189\tbatch_accuracy: 96.09%\n",
            "Ep: 15/20\tIt: 401/468\tbatch_loss: 0.0563\tbatch_accuracy: 99.22%\n",
            "Ep: 15/20\tIt: 451/468\tbatch_loss: 0.0767\tbatch_accuracy: 97.66%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ep: 15/20\tIt: 468/468\tbatch_loss: 0.1143\tbatch_accuracy: 96.09%\n",
            "Test acc: 98.45%\tTest loss: 0.0490\n",
            "Train Confusion Matrix:\n",
            "[[ 973    0    1    0    1    1    1    1    2    0]\n",
            " [   0 1124    3    5    1    0    2    0    0    0]\n",
            " [   0    1 1022    1    1    0    0    4    3    0]\n",
            " [   0    0    0  996    0    6    0    4    2    2]\n",
            " [   1    0    0    0  957    0    5    1    1   17]\n",
            " [   2    0    1    8    0  875    1    2    1    2]\n",
            " [   4    1    0    0    1    7  943    0    2    0]\n",
            " [   1    2    7    2    0    1    0 1007    2    6]\n",
            " [   2    0    0    3    1    2    1    2  960    3]\n",
            " [   1    0    0    3    7    2    0    4    4  988]]\n",
            "Best test acc: 98.45%\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ep: 16/20\tIt: 1/468\tbatch_loss: 0.0860\tbatch_accuracy: 98.44%\n",
            "Ep: 16/20\tIt: 51/468\tbatch_loss: 0.0865\tbatch_accuracy: 97.66%\n",
            "Ep: 16/20\tIt: 101/468\tbatch_loss: 0.0728\tbatch_accuracy: 99.22%\n",
            "Ep: 16/20\tIt: 151/468\tbatch_loss: 0.0727\tbatch_accuracy: 97.66%\n",
            "Ep: 16/20\tIt: 201/468\tbatch_loss: 0.0676\tbatch_accuracy: 98.44%\n",
            "Ep: 16/20\tIt: 251/468\tbatch_loss: 0.0997\tbatch_accuracy: 96.88%\n",
            "Ep: 16/20\tIt: 301/468\tbatch_loss: 0.0820\tbatch_accuracy: 97.66%\n",
            "Ep: 16/20\tIt: 351/468\tbatch_loss: 0.0908\tbatch_accuracy: 96.09%\n",
            "Ep: 16/20\tIt: 401/468\tbatch_loss: 0.1637\tbatch_accuracy: 96.09%\n",
            "Ep: 16/20\tIt: 451/468\tbatch_loss: 0.0853\tbatch_accuracy: 96.09%\n",
            "Ep: 16/20\tIt: 468/468\tbatch_loss: 0.0967\tbatch_accuracy: 96.88%\n",
            "Test acc: 98.34%\tTest loss: 0.0508\n",
            "Train Confusion Matrix:\n",
            "[[ 975    0    1    0    1    0    2    1    0    0]\n",
            " [   2 1121    3    4    0    0    4    1    0    0]\n",
            " [   6    1 1013    2    0    0    2    2    4    2]\n",
            " [   0    0    4  998    0    4    0    2    2    0]\n",
            " [   1    0    0    0  960    0    7    0    1   13]\n",
            " [   3    1    0   11    0  874    2    1    0    0]\n",
            " [   1    1    0    0    2    3  950    0    1    0]\n",
            " [   0    2    6    3    2    1    0 1002    2   10]\n",
            " [   3    0    1    2    1    4    4    1  953    5]\n",
            " [   1    1    0    1    9    5    0    3    1  988]]\n",
            "Best test acc: 98.45%\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ep: 17/20\tIt: 1/468\tbatch_loss: 0.0665\tbatch_accuracy: 96.88%\n",
            "Ep: 17/20\tIt: 51/468\tbatch_loss: 0.0799\tbatch_accuracy: 97.66%\n",
            "Ep: 17/20\tIt: 101/468\tbatch_loss: 0.0449\tbatch_accuracy: 99.22%\n",
            "Ep: 17/20\tIt: 151/468\tbatch_loss: 0.0360\tbatch_accuracy: 98.44%\n",
            "Ep: 17/20\tIt: 201/468\tbatch_loss: 0.0573\tbatch_accuracy: 99.22%\n",
            "Ep: 17/20\tIt: 251/468\tbatch_loss: 0.0908\tbatch_accuracy: 98.44%\n",
            "Ep: 17/20\tIt: 301/468\tbatch_loss: 0.0745\tbatch_accuracy: 97.66%\n",
            "Ep: 17/20\tIt: 351/468\tbatch_loss: 0.1738\tbatch_accuracy: 93.75%\n",
            "Ep: 17/20\tIt: 401/468\tbatch_loss: 0.0245\tbatch_accuracy: 100.00%\n",
            "Ep: 17/20\tIt: 451/468\tbatch_loss: 0.0773\tbatch_accuracy: 97.66%\n",
            "Ep: 17/20\tIt: 468/468\tbatch_loss: 0.0443\tbatch_accuracy: 97.66%\n",
            "Test acc: 98.45%\tTest loss: 0.0474\n",
            "Train Confusion Matrix:\n",
            "[[ 977    0    1    0    0    1    0    1    0    0]\n",
            " [   0 1127    2    3    0    0    0    3    0    0]\n",
            " [   2    0 1022    2    0    0    0    5    1    0]\n",
            " [   0    0    5  983    0   13    0    7    2    0]\n",
            " [   1    1    2    0  963    0    3    1    2    9]\n",
            " [   2    1    1    4    0  880    1    2    0    1]\n",
            " [   7    2    0    0    3    4  938    0    4    0]\n",
            " [   0    0    4    0    0    1    0 1020    2    1]\n",
            " [   5    0    2    0    0    2    0    1  961    3]\n",
            " [   6    0    0    1    9    7    0    6    6  974]]\n",
            "Best test acc: 98.45%\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ep: 18/20\tIt: 1/468\tbatch_loss: 0.0679\tbatch_accuracy: 97.66%\n",
            "Ep: 18/20\tIt: 51/468\tbatch_loss: 0.0455\tbatch_accuracy: 99.22%\n",
            "Ep: 18/20\tIt: 101/468\tbatch_loss: 0.0523\tbatch_accuracy: 97.66%\n",
            "Ep: 18/20\tIt: 151/468\tbatch_loss: 0.0632\tbatch_accuracy: 97.66%\n",
            "Ep: 18/20\tIt: 201/468\tbatch_loss: 0.0485\tbatch_accuracy: 98.44%\n",
            "Ep: 18/20\tIt: 251/468\tbatch_loss: 0.0540\tbatch_accuracy: 98.44%\n",
            "Ep: 18/20\tIt: 301/468\tbatch_loss: 0.0583\tbatch_accuracy: 96.88%\n",
            "Ep: 18/20\tIt: 351/468\tbatch_loss: 0.1741\tbatch_accuracy: 95.31%\n",
            "Ep: 18/20\tIt: 401/468\tbatch_loss: 0.0565\tbatch_accuracy: 97.66%\n",
            "Ep: 18/20\tIt: 451/468\tbatch_loss: 0.0789\tbatch_accuracy: 96.09%\n",
            "Ep: 18/20\tIt: 468/468\tbatch_loss: 0.1127\tbatch_accuracy: 97.66%\n",
            "Test acc: 98.65%\tTest loss: 0.0400\n",
            "Train Confusion Matrix:\n",
            "[[ 972    0    2    1    0    1    3    1    0    0]\n",
            " [   2 1125    2    1    1    0    1    3    0    0]\n",
            " [   3    1 1023    1    0    0    0    2    2    0]\n",
            " [   0    1    3  997    0    4    0    3    2    0]\n",
            " [   1    0    2    0  964    0    3    0    4    8]\n",
            " [   0    1    1    5    0  881    2    1    0    1]\n",
            " [   4    2    0    0    2    3  946    0    1    0]\n",
            " [   0    0    8    1    2    0    0 1011    2    4]\n",
            " [   4    0    3    0    0    3    3    0  958    3]\n",
            " [   1    0    0    0   11    3    0    3    3  988]]\n",
            "Best test acc: 98.65%\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ep: 19/20\tIt: 1/468\tbatch_loss: 0.0606\tbatch_accuracy: 99.22%\n",
            "Ep: 19/20\tIt: 51/468\tbatch_loss: 0.0695\tbatch_accuracy: 97.66%\n",
            "Ep: 19/20\tIt: 101/468\tbatch_loss: 0.0897\tbatch_accuracy: 98.44%\n",
            "Ep: 19/20\tIt: 151/468\tbatch_loss: 0.0910\tbatch_accuracy: 95.31%\n",
            "Ep: 19/20\tIt: 201/468\tbatch_loss: 0.0116\tbatch_accuracy: 100.00%\n",
            "Ep: 19/20\tIt: 251/468\tbatch_loss: 0.0639\tbatch_accuracy: 97.66%\n",
            "Ep: 19/20\tIt: 301/468\tbatch_loss: 0.0686\tbatch_accuracy: 97.66%\n",
            "Ep: 19/20\tIt: 351/468\tbatch_loss: 0.0191\tbatch_accuracy: 99.22%\n",
            "Ep: 19/20\tIt: 401/468\tbatch_loss: 0.0952\tbatch_accuracy: 96.09%\n",
            "Ep: 19/20\tIt: 451/468\tbatch_loss: 0.0576\tbatch_accuracy: 98.44%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ep: 19/20\tIt: 468/468\tbatch_loss: 0.0729\tbatch_accuracy: 98.44%\n",
            "Test acc: 98.85%\tTest loss: 0.0365\n",
            "Train Confusion Matrix:\n",
            "[[ 976    0    1    0    1    0    1    1    0    0]\n",
            " [   0 1127    2    1    1    0    1    3    0    0]\n",
            " [   2    0 1024    1    0    0    0    5    0    0]\n",
            " [   0    0    2 1000    0    2    0    4    2    0]\n",
            " [   1    0    0    0  964    0    4    0    2   11]\n",
            " [   3    0    0    8    0  876    1    2    0    2]\n",
            " [   4    2    0    0    1    2  948    0    1    0]\n",
            " [   0    1    3    1    0    0    0 1016    2    5]\n",
            " [   5    0    1    0    0    3    1    1  960    3]\n",
            " [   1    0    0    0    6    2    0    4    2  994]]\n",
            "Best test acc: 98.85%\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ep: 20/20\tIt: 1/468\tbatch_loss: 0.0806\tbatch_accuracy: 96.09%\n",
            "Ep: 20/20\tIt: 51/468\tbatch_loss: 0.0251\tbatch_accuracy: 99.22%\n",
            "Ep: 20/20\tIt: 101/468\tbatch_loss: 0.0690\tbatch_accuracy: 97.66%\n",
            "Ep: 20/20\tIt: 151/468\tbatch_loss: 0.0219\tbatch_accuracy: 99.22%\n",
            "Ep: 20/20\tIt: 201/468\tbatch_loss: 0.0498\tbatch_accuracy: 97.66%\n",
            "Ep: 20/20\tIt: 251/468\tbatch_loss: 0.0507\tbatch_accuracy: 99.22%\n",
            "Ep: 20/20\tIt: 301/468\tbatch_loss: 0.0708\tbatch_accuracy: 97.66%\n",
            "Ep: 20/20\tIt: 351/468\tbatch_loss: 0.0407\tbatch_accuracy: 99.22%\n",
            "Ep: 20/20\tIt: 401/468\tbatch_loss: 0.0249\tbatch_accuracy: 98.44%\n",
            "Ep: 20/20\tIt: 451/468\tbatch_loss: 0.0293\tbatch_accuracy: 99.22%\n",
            "Ep: 20/20\tIt: 468/468\tbatch_loss: 0.0481\tbatch_accuracy: 98.44%\n",
            "Test acc: 98.89%\tTest loss: 0.0326\n",
            "Train Confusion Matrix:\n",
            "[[ 976    0    1    0    1    0    1    1    0    0]\n",
            " [   0 1127    1    3    0    0    1    3    0    0]\n",
            " [   1    0 1024    1    0    0    0    4    2    0]\n",
            " [   0    1    2 1000    0    3    0    2    2    0]\n",
            " [   1    0    0    0  965    0    4    0    3    9]\n",
            " [   0    0    0    6    0  880    1    2    1    2]\n",
            " [   4    2    0    0    1    4  946    0    1    0]\n",
            " [   0    1    5    2    1    1    0 1014    1    3]\n",
            " [   3    0    1    0    0    2    1    0  964    3]\n",
            " [   1    0    0    1    7    3    0    1    3  993]]\n",
            "Best test acc: 98.89%\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train acc: 98.69%\tTrain loss: 0.0435\n",
            "Train Confusion Matrix:\n",
            "[[5892    0    4    0    1    2    9    0    5    3]\n",
            " [   0 6678   21    7    5    1    2   13    1    2]\n",
            " [   4   11 5881    8    3    0    3   17   23    3]\n",
            " [   3    4   25 6017    0   21    1   23   18   10]\n",
            " [   0    6    7    0 5742    1   10    1    6   52]\n",
            " [   7    3    2   24    1 5316   34    2   16    5]\n",
            " [   7    4    1    0    7    6 5878    0    7    1]\n",
            " [   1   15   24    7   16    1    0 6150    4   38]\n",
            " [   5    7   13    3    6   19   18    5 5746   17]\n",
            " [   5    3    2    9   46   11    2   26   19 5819]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test acc: 98.89%\tTest loss: 0.0326\n",
            "Train Confusion Matrix:\n",
            "[[ 976    0    1    0    1    0    1    1    0    0]\n",
            " [   0 1127    1    3    0    0    1    3    0    0]\n",
            " [   1    0 1024    1    0    0    0    4    2    0]\n",
            " [   0    1    2 1000    0    3    0    2    2    0]\n",
            " [   1    0    0    0  965    0    4    0    3    9]\n",
            " [   0    0    0    6    0  880    1    2    1    2]\n",
            " [   4    2    0    0    1    4  946    0    1    0]\n",
            " [   0    1    5    2    1    1    0 1014    1    3]\n",
            " [   3    0    1    0    0    2    1    0  964    3]\n",
            " [   1    0    0    1    7    3    0    1    3  993]]\n",
            "Ended at 2024-05-11 06:35:41\n",
            "Duration: 0:12:12.720524\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class AllArguments:\n",
        "    def __init__(self):\n",
        "        # Training Arguments\n",
        "        self.epochs = 20\n",
        "        self.warmup_epochs = 10\n",
        "        self.batch_size = 128\n",
        "        self.n_classes = 10\n",
        "        self.n_workers = 4\n",
        "        self.lr = 5e-4\n",
        "        self.output_path = './outputs'\n",
        "\n",
        "        # Data arguments\n",
        "        self.dataset = 'mnist'\n",
        "        self.image_size = 28\n",
        "        self.patch_size = 4\n",
        "        self.n_channels = 1\n",
        "        self.data_path = './data/'\n",
        "\n",
        "        # ViT Arguments\n",
        "        self.embed_dim = 64\n",
        "        self.n_attention_heads = 4\n",
        "        self.forward_mul = 2\n",
        "        self.n_layers = 6\n",
        "        self.dropout = 0.1\n",
        "        self.model_path = './model'\n",
        "        self.load_model = False\n",
        "\n",
        "        #\n",
        "        self.model_path = os.path.join(self.model_path, self.dataset)\n",
        "        self.output_path = os.path.join(self.output_path, self.dataset)\n",
        "        self.n_patches = (self.image_size // self.patch_size) ** 2\n",
        "        self.is_cuda = torch.cuda.is_available() # Check GPU availability\n",
        "\n",
        "args = AllArguments()\n",
        "\n",
        "start_time = datetime.datetime.now()\n",
        "print(\"Started at \" + str(start_time.strftime('%Y-%m-%d %H:%M:%S')))\n",
        "\n",
        "\n",
        "main(args)\n",
        "\n",
        "end_time = datetime.datetime.now()\n",
        "duration = end_time - start_time\n",
        "print(\"Ended at \" + str(end_time.strftime('%Y-%m-%d %H:%M:%S')))\n",
        "print(\"Duration: \" + str(duration))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZM0ywvJPavpv",
        "outputId": "18f70906-e8e9-4fee-ff22-ac0358981f7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using GPU\n",
            "--------Network--------\n",
            "VisionTransformer(\n",
            "  (embedding): EmbedLayer(\n",
            "    (conv1): Conv2d(1, 64, kernel_size=(4, 4), stride=(4, 4))\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (encoder): ModuleList(\n",
            "    (0-5): 6 x Encoder(\n",
            "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      (attention): SelfAttention(\n",
            "        (queries): Linear(in_features=64, out_features=64, bias=True)\n",
            "        (keys): Linear(in_features=64, out_features=64, bias=True)\n",
            "        (values): Linear(in_features=64, out_features=64, bias=True)\n",
            "        (out_projection): Linear(in_features=64, out_features=64, bias=True)\n",
            "      )\n",
            "      (dropout1): Dropout(p=0.1, inplace=False)\n",
            "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      (fc1): Linear(in_features=64, out_features=128, bias=True)\n",
            "      (activation): GELU(approximate='none')\n",
            "      (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
            "      (dropout2): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "  (classifier): Classifier(\n",
            "    (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
            "    (activation): Tanh()\n",
            "    (fc2): Linear(in_features=64, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "Using pretrained model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x600 with 10 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "args = AllArguments()\n",
        "args.load_model=True\n",
        "args.batch_size=10\n",
        "# args.n_workers=1\n",
        "solver = Solver(args)\n",
        "solver.plot_mnist_predictions()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
